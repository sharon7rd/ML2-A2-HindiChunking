{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9b999796-853d-4489-a1ed-9c10b14d92a0",
   "metadata": {},
   "source": [
    "SETUP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c8e87932-bc40-432d-8279-1579c5046833",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing libraries\n",
    "import os\n",
    "import re #regex for chunk id extraction\n",
    "import numpy as np\n",
    "from datasets import load_dataset, Dataset, DatasetDict\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForTokenClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorForTokenClassification)\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68721f45-d9ee-4a83-a21a-ef72f5491e8a",
   "metadata": {},
   "source": [
    "PART 1: TOKEN CLASSIFICATION\n",
    "- Replicating the hugging face token classification tutorial using DIstilBERT on the English dataset ( first understand token classification that's transformer based before adapting it to Hindi data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d94e70c1-74de-4e38-a45e-5fd2b9f14ecc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#Loading dataset\n",
    "#conll2003 = load_dataset(\"conll2003\")\n",
    "conll2003 = load_dataset(\"conll2003\", trust_remote_code=True)\n",
    "\n",
    "# extracting label names from the dataset feats\n",
    "label_names = conll2003[\"train\"].features[\"ner_tags\"].feature.names\n",
    "#bidirectional mapping\n",
    "id2label = {i: l for i, l in enumerate(label_names) }\n",
    "label2id = {l: i for i, l in enumerate(label_names) }\n",
    "#loading distilbBERT tokeniser\n",
    "tokenizer_en = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "#note to self:  4 entities PER, ORG, LOC, MISC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fd5a4e88-11bb-4f11-8998-d0e2a7468606",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenisation and Label alignement \n",
    "\n",
    "#need to use subword tokenisation as labels are at word lvl, thus we need to tokenise words that are pre-split and align labels with subword tokens \n",
    "def tokenize_and_align(examples):\n",
    "    #tokenising pre-split words\n",
    "    tokenized = tokenizer_en(\n",
    "        examples[\"tokens\"],\n",
    "        is_split_into_words=True,\n",
    "        truncation=True\n",
    "    )\n",
    "    labels = []\n",
    "    for i, label_seq in enumerate(examples[\"ner_tags\"]):\n",
    "        word_ids = tokenized.word_ids(i) #mapping each token back to org word\n",
    "        #return none for special tokens \n",
    "        label_ids = []\n",
    "        prev = None\n",
    "        for w in word_ids:\n",
    "            if  w is None:\n",
    "                label_ids.append(-100) #ignore special tokens (eg: CLS, SEP etc)\n",
    "            elif w != prev:\n",
    "                label_ids.append(label_seq[w]) #first subword of a word should use word's label\n",
    "            else:\n",
    "                label_ids.append(-100) #ignore eg: -ing etc\n",
    "            prev = w\n",
    "        labels.append(label_ids)\n",
    "        #adding aligned labels to tokenised output\n",
    "    tokenized[\"labels\"] = labels\n",
    "    return tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6de5ceaa-01d9-4947-af58-81a597898225",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ec7f298288e4922ab457e869582a0a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/14041 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7684eb88ba4b459f978de07a139c6243",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3250 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "171e22cdb36b4b4ea346e29f163a723b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3453 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#applying tokenisation to the dataset\n",
    "tokenized_conll = conll2003.map(\n",
    "    tokenize_and_align,\n",
    "    batched=True,\n",
    "    remove_columns=conll2003[\"train\"].column_names\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9707f975-25b8-4242-b435-3ddcb19b6b8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForTokenClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "#need to initialise Model\n",
    "#Loading DistilBERT for tokenclassification\n",
    "model_p1 = AutoModelForTokenClassification.from_pretrained(\n",
    "    \"distilbert-base-uncased\",\n",
    "    num_labels=len(label_names),\n",
    "    id2label=id2label,\n",
    "    label2id=label2id\n",
    ")\n",
    "\n",
    "#from seqeval.metrics import f1_score\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "#F1 macro score for token classification\n",
    "def compute_metrics(p):\n",
    "    preds = np.argmax(p.predictions, axis=2)\n",
    "    labels = p.label_ids\n",
    "#flattening + filtering out ignored tokens/subwords (-100)\n",
    "    all_preds  = []\n",
    "    all_labels = []\n",
    "\n",
    "    for pred, lab in zip(preds, labels):\n",
    "        for p_i, l_i in zip(pred, lab):\n",
    "            if l_i != -100: \n",
    "                all_preds.append (p_i)\n",
    "                all_labels.append(l_i)\n",
    "    return {\n",
    "        \"f1_macro\": f1_score (all_labels, all_preds, average=\"macro\")\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "172e5330-a479-40a1-ac73-38ea74acf676",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "/usr/local/lib64/python3.12/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='660' max='660' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [660/660 03:12, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1 Macro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.053084</td>\n",
       "      <td>0.910687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.048469</td>\n",
       "      <td>0.926186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.098400</td>\n",
       "      <td>0.048771</td>\n",
       "      <td>0.931853</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib64/python3.12/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib64/python3.12/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib64/python3.12/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib64/python3.12/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib64/python3.12/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='108' max='54' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [54/54 00:13]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib64/python3.12/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Part 1 Results (English NER)\n",
      "Test F1 Macro: 0.8836\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#Training the model\n",
    "trainer_p1 = Trainer(\n",
    "    model = model_p1,\n",
    "    args= TrainingArguments(\n",
    "        output_dir= \"./p1\",\n",
    "        evaluation_strategy = \"epoch\",\n",
    "        num_train_epochs = 3,\n",
    "        per_device_train_batch_size = 16,\n",
    "        per_device_eval_batch_size = 16,\n",
    "    ),\n",
    "    train_dataset = tokenized_conll[\"train\"],\n",
    "    eval_dataset = tokenized_conll[\"validation\"],\n",
    "    tokenizer = tokenizer_en,\n",
    "    data_collator = DataCollatorForTokenClassification(tokenizer_en), #to handle padding\n",
    "    compute_metrics = compute_metrics,\n",
    ")\n",
    "\n",
    "trainer_p1.train()\n",
    "trainer_p1.evaluate(tokenized_conll[\"test\"])\n",
    "results_p1 = trainer_p1.evaluate(tokenized_conll[\"test\"])\n",
    "\n",
    "print(f\"\\n Part 1 Results (English NER)\")\n",
    "print(f\"Test F1 Macro: {results_p1['eval_f1_macro']:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba0ed003-dca7-4853-8f31-919b042f5325",
   "metadata": {},
   "source": [
    "PART 2: HINDI CHUNKING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c2a54f3-5a61-4f0f-b50b-f4a04448e242",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adapting the token classificaiton from earlier to Hindi chunking. IOB chunking \n",
    "#note to self: Beginning (B), Inside (I), Outside (O) \n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "from datasets import Dataset, DatasetDict\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForTokenClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorForTokenClassification\n",
    ")\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2560eff2-5f22-413d-af45-b84cb1f69405",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = \"/srv/data/lt2326-h25/a2\"\n",
    "train_file = os.path.join(DATA_DIR, \"hi_hdtb-ud-train.conllu\")\n",
    "dev_file   = os.path.join(DATA_DIR, \"hi_hdtb-ud-dev.conllu\")\n",
    "test_file  = os.path.join(DATA_DIR, \"hi_hdtb-ud-test.conllu\")\n",
    "\n",
    "#parsing Hindi conllu file into (return)sentences w/ tokens and chunk info\n",
    "def parse_hindi_conll(filepath):\n",
    "    sentences = []\n",
    "    current = {'tokens': [], 'chunk_ids': [], 'raw_chunk_ids': []}\n",
    "\n",
    "    with open(filepath, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            #empty line -> sentence boundary\n",
    "            if not line:\n",
    "                if current['tokens']: #saving sentence if not emprty\n",
    "                    sentences.append(current)\n",
    "                    current = { 'tokens': [], 'chunk_ids': [], 'raw_chunk_ids': []}\n",
    "                continue\n",
    "            if line.startswith('#'): #Skipping comment lines\n",
    "                continue  \n",
    "#split line to columns inorder to handle spaces and tabs\n",
    "            parts = line.split('\\t') if '\\t' in line else line.split()\n",
    "            if len(parts) < 2:\n",
    "                continue\n",
    "            token = parts[1]\n",
    "\n",
    "            #extracting chunk info using regex. eg: ChunkId=NP2 -> 'NP' and '2' (seperately)\n",
    "            chunk_match = re.search(r'ChunkId=([A-Za-z]+)(\\d*)', line)\n",
    "            if chunk_match:\n",
    "                raw_chunk = chunk_match.group(1) + chunk_match.group(2)  # NP2\n",
    "                chunk_type = chunk_match.group(1)  # NP\n",
    "            else:\n",
    "                raw_chunk = 'O'\n",
    "                chunk_type = 'O'\n",
    "\n",
    "            current['tokens'].append(token)\n",
    "            current['chunk_ids'].append(chunk_type)\n",
    "            current['raw_chunk_ids'].append(raw_chunk)\n",
    "    if current['tokens']:\n",
    "        sentences.append(current)\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4f4bc4e-bbb1-4ff0-92b4-8103c6f76b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#converting chunk IDs to  IOB form\n",
    "#note to self: first token of chunk (B), continuation token (I), and tokens o/s chunks (O)\n",
    "\n",
    "def make_iob(sentences):\n",
    "    for sent in sentences:\n",
    "        iob = []\n",
    "        prev_raw = None #in order to detect boundaries\n",
    "        for chunk_type, raw_chunk in zip(sent['chunk_ids'], sent['raw_chunk_ids']):\n",
    "            if chunk_type == 'O':\n",
    "                iob.append('O')\n",
    "            elif raw_chunk != prev_raw:\n",
    "                iob.append(f'B-{chunk_type}')\n",
    "            else:\n",
    "                iob.append(f'I-{chunk_type}')\n",
    "            prev_raw = raw_chunk\n",
    "        sent['iob_labels'] = iob\n",
    "    return sentences\n",
    "\n",
    "train_sents = make_iob(parse_hindi_conll(train_file))\n",
    "dev_sents   = make_iob(parse_hindi_conll(dev_file))\n",
    "test_sents  = make_iob(parse_hindi_conll(test_file))\n",
    "print(f\"Train sentences: {len(train_sents)}\")\n",
    "print(f\"Dev sentences: {len(dev_sents)}\")\n",
    "print(f\"Test sentences: {len(test_sents)}\")\n",
    "\n",
    "# Example sentence inorder to verify formatting (IOB)\n",
    "example = train_sents[0]\n",
    "print(\"\\n Example sentence with IOB labels: \")\n",
    "for tok, label in zip(example['tokens'], example['iob_labels']):\n",
    "    print(f\"{tok:10s} -> {label}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58eb6720-4b13-4bb7-924d-80b6f7f657d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creatiing bidirectional label mappings for model\n",
    "\n",
    "labels = sorted({label for sent in train_sents for label in sent[\"iob_labels\"]}) #only use trianing labels\n",
    "label2id = {label: idx for idx, label in enumerate(labels)}\n",
    "id2label = {idx: label for label, idx in label2id.items()}\n",
    "print(\"\\n Labels:\", labels)\n",
    "print( f\"Total number of labels: {len(labels)}\")\n",
    "print(\"Label2ID:\", label2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d331845-75d9-456e-9ad3-5abefe639762",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Conv parsed sentences to Hugging face datset format\n",
    "def to_dataset(sents):\n",
    "    return Dataset.from_dict({\n",
    "        \"tokens\": [s[\"tokens\"] for s in sents],\n",
    "        \"labels\": [[label2id[l] for l in s[\"iob_labels\"]] for s in sents] #conv to numeric ids\n",
    "    })\n",
    "data = DatasetDict({\n",
    "    \"train\": to_dataset(train_sents),\n",
    "    \"validation\": to_dataset(dev_sents),\n",
    "    \"test\": to_dataset(test_sents)\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e779d8b9-9a64-4f1b-a3cc-6cff0b68c567",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Model 1: mBERT\n",
    "#loading tokeniser\n",
    "tokenizer_hi = AutoTokenizer.from_pretrained(\"bert-base-multilingual-cased\")\n",
    "\n",
    "#tokenise hindi text + align labels with subword token\n",
    "def tokenize_hindi(examples):\n",
    "    tok = tokenizer_hi(\n",
    "        examples[\"tokens\"],\n",
    "        is_split_into_words=True,\n",
    "        truncation=True\n",
    "    )\n",
    "    aligned = []\n",
    "    \n",
    "    for i, labs in enumerate(examples[\"labels\"]):\n",
    "        word_ids = tok.word_ids(i)\n",
    "        ids, prev = [], None\n",
    "        for w in word_ids:\n",
    "            if w is None:\n",
    "                ids.append(-100)\n",
    "            elif w != prev:\n",
    "                ids.append(labs[w])\n",
    "            else:\n",
    "                ids.append(-100)\n",
    "            prev = w\n",
    "        aligned.append(ids)\n",
    "    \n",
    "    tok[\"labels\"] = aligned\n",
    "    return tok\n",
    "#applying tokenisaiton to all splits\n",
    "tokenized_hi = data.map(tokenize_hindi, batched=True, remove_columns=[\"tokens\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccfd47bd-d00c-4e4b-8cd7-91d8ffa13866",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_hi = AutoModelForTokenClassification.from_pretrained(\n",
    "    \"bert-base-multilingual-cased\",\n",
    "    num_labels = len(labels),\n",
    "    id2label = id2label,\n",
    "    label2id = label2id\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16966267-995c-4962-a74a-2bf16a098529",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def compute_metrics(p):\n",
    "    preds = np.argmax(p.predictions, axis=2)\n",
    "    labels = p.label_ids\n",
    "\n",
    "    all_preds, all_labels = [], []\n",
    "    for pred, lab in zip(preds, labels):\n",
    "        for p_i, l_i in zip(pred, lab):\n",
    "            if l_i != -100:\n",
    "                all_preds.append(p_i)\n",
    "                all_labels.append(l_i)\n",
    "    return {\"f1_macro\": f1_score(all_labels, all_preds, average=\"macro\")}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5fc6e87-c441-4d05-8142-99b6a5b55767",
   "metadata": {},
   "outputs": [],
   "source": [
    "#trainer for mBERT\n",
    "trainer_hi = Trainer(\n",
    "    model = model_hi,\n",
    "    args = TrainingArguments(\n",
    "        output_dir =\"./hindi\",\n",
    "        evaluation_strategy = \"epoch\",\n",
    "        num_train_epochs = 5,\n",
    "        per_device_train_batch_size = 16,\n",
    "        per_device_eval_batch_size = 16,\n",
    "        save_strategy = \"epoch\"\n",
    "    ),\n",
    "    \n",
    "    train_dataset = tokenized_hi[\"train\"],\n",
    "    eval_dataset = tokenized_hi[\"validation\"],\n",
    "    tokenizer = tokenizer_hi,\n",
    "    data_collator = DataCollatorForTokenClassification(tokenizer_hi),\n",
    "    compute_metrics = compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8a79c74-a9e6-4d07-bded-587c1fbc8808",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train\n",
    "trainer_hi.train()\n",
    "results_hi = trainer_hi.evaluate(tokenized_hi[\"test\"])\n",
    "print(f\"\\n mBERT Test F1 Macro:  {results_hi['eval_f1_macro']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "009a7287-8cb9-4c75-835c-01240230eeb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#MODEL 2 - DistilBERT Multilinguial (for faster training)\n",
    "#loading tokeniser\n",
    "tokenizer_distil = AutoTokenizer.from_pretrained(\"distilbert-base-multilingual-cased\")\n",
    "#follow same alignment as mBERT\n",
    "def tokenize_hindi_distil(examples):\n",
    "    tok = tokenizer_distil(\n",
    "        examples[\"tokens\"],\n",
    "        is_split_into_words=True,\n",
    "        truncation=True\n",
    "    )\n",
    "    \n",
    "    aligned = []\n",
    "    for i, labs in enumerate(examples[\"labels\"]):\n",
    "        word_ids = tok.word_ids(i)\n",
    "        ids, prev = [], None\n",
    "        \n",
    "        for w in word_ids:\n",
    "            if w is None:\n",
    "                ids.append(-100)\n",
    "            elif w != prev:\n",
    "                ids.append(labs[w])\n",
    "            else:\n",
    "                ids.append(-100)\n",
    "            prev = w\n",
    "        aligned.append(ids)\n",
    "    tok[\"labels\"] = aligned\n",
    "    return tok\n",
    "#tokenising dataset\n",
    "tokenized_hi_distil = data.map(tokenize_hindi_distil, batched=True, remove_columns=[\"tokens\"])\n",
    "\n",
    "#initialisation\n",
    "model_distil = AutoModelForTokenClassification.from_pretrained(\n",
    "    \"distilbert-base-multilingual-cased\",\n",
    "    num_labels=len(labels),\n",
    "    id2label=id2label,\n",
    "    label2id=label2id\n",
    ")\n",
    "\n",
    "#TRainer\n",
    "trainer_distil = Trainer(\n",
    "    model=model_distil,\n",
    "    args=TrainingArguments(\n",
    "        output_dir=\"./hindi_distilbert\",\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        num_train_epochs=5,\n",
    "        per_device_train_batch_size=16,\n",
    "        per_device_eval_batch_size=16,\n",
    "    ),\n",
    "    train_dataset=tokenized_hi_distil[\"train\"],\n",
    "    eval_dataset=tokenized_hi_distil[\"validation\"],\n",
    "    tokenizer=tokenizer_distil,\n",
    "    data_collator=DataCollatorForTokenClassification(tokenizer_distil),\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer_distil.train()\n",
    "results_distil = trainer_distil.evaluate(tokenized_hi_distil[\"test\"])\n",
    "\n",
    "print(\"\\n Model Comparison on Hindi Chunking:\")\n",
    "print(f\"mBERT: F1 = {results_hi['eval_f1_macro']:.4f}\")\n",
    "print(f\"DistillBERT: F1 = {results_distil['eval_f1_macro']:.4f}\")\n",
    "\n",
    "print(f\"Performance gap: {(results_hi['eval_f1_macro'] - results_distil['eval_f1_macro']):.4f} \"\n",
    "      f\"({((results_hi['eval_f1_macro'] - results_distil['eval_f1_macro']) / results_distil['eval_f1_macro'] * 100):.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3449c293-902c-4603-aa50-190b4922dfb5",
   "metadata": {},
   "source": [
    "PART 3: PERFORMANCE ANALYSIS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7934a8b-cf83-412c-a118-e74653426e66",
   "metadata": {},
   "source": [
    "- Both models were evaluated on Hindi IOB chunking, identifying chunk boundaries using BIO labels.\n",
    "\n",
    "Evaluation Metric:\n",
    "- I used macro F1 score as it averages F1 across all the 3 labels equally (regardless of class frequency). Additionally, it seemed ideal for IOB tasks cause it ensured balances performance across all label types, thus preventing/reducing bias towards the majority class \n",
    "\n",
    "Individual Model Analysis:\n",
    "- mBERT F1 = 0.8995\n",
    "    - The training progress showed steady imporvement eg: starting at 0.824 (epoch 1) and then hightening at 0.907 (epoch 4).\n",
    "    - Validation loss decreased consistently from 0.076 to 0.059, and then slightly increasing to 0.064\n",
    "    - Overall, the model demonstrated effective transfer learning from multilingual to Hindi\n",
    "\n",
    "- DistilBERT F1 = 0.8716\n",
    "    - Significantly smaller than mBERT in size\n",
    "    - Training progression was slower in comparison eg: starting at 0.801 (iepoch 1) and 0.867 (epoch 4)\n",
    "    - While validation loss decreased from 0.105 to 0.075\n",
    "    - It had an overall 3.2% performance gap in comparison to mBERT\n",
    "\n",
    "Comparative Analysis:\n",
    "- Performance: mBERT clearly outperformed DistilBERT witha 3.2% relative difference (0.8995 vs 0.8716). Such performance gap is consistence and typical when comparing full BERT models to their respective distillled verisons due to eg: model capacity/parameters, attention mechanisms (full attention layers are better at capturing long range dependencies in Hindi syntax) etc\n",
    "- However, despite lower performance, DistilBERT does offer more practical advantages such as:\n",
    "    - Faster training time and inference speed (faster predictions)\n",
    "    - Being a smaller model size means that it enables deployments in resource-constrained (?) environments\n",
    "\n",
    "- Trade-offs: \n",
    "    - mBERT: when maximum accuracy is critical and if there is sufficent computational resources available\n",
    "    - DistilBERT: when fast trianing or inference is prioritised and if memory is limited\n",
    "    - Essentially, accuracy vs efficiency\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdff726b-ebb3-4725-9d9e-f3ab14e117eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisations\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#plotting training curves comparison\n",
    "epochs = [1, 2, 3, 4, 5]\n",
    "mbert_scores = [0.824197, 0.837153, 0.899832, 0.906529, 0.904749]\n",
    "distilbert_scores = [0.800774, 0.826090, 0.834281, 0.867580, 0.866282]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(epochs, mbert_scores, 'b-o', label = 'mBERT', linewidth=2)\n",
    "plt.plot(epochs, distilbert_scores, 'r-s', label = 'DistilBERT-multilingual', linewidth=2)\n",
    "plt.xlabel('Epoch', fontsize=12)\n",
    "plt.ylabel('F1 Macro Score', fontsize=12)\n",
    "plt.title('Training Progression: mBERT vs DistilBERT on Hindi Chuking', fontsize=14)\n",
    "plt.legend(fontsize = 11)\n",
    "plt.grid(True, alpha = 0.3)\n",
    "plt.ylim(0.75, 0.95)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "print(\"\\n Final Smmary of  Test Results:\")\n",
    "print(f\"mBERT: F1 = {results_hi['eval_f1_macro']:.4f}\")\n",
    "print(f\"DistilBERT: F1 = {results_distil['eval_f1_macro']:.4f}\")\n",
    "print(f\"Performance gap:{(results_hi['eval_f1_macro'] - results_distil['eval_f1_macro']):.4f} ({((results_hi['eval_f1_macro'] - results_distil['eval_f1_macro']) / results_distil['eval_f1_macro'] * 100):.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "922c92b2-a5df-4c4d-a96c-9bede4f39598",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
